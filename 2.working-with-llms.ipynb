{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Working with Large Language Models (LLMs) via API\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Today we'll learn how to work with Large Language Models (LLMs) through their APIs. By the end of this lecture, you'll be able to:\n",
    "\n",
    "1. **Use Provider SDKs** - Call OpenAI, Anthropic, and Google directly\n",
    "2. **Use LiteLLM** - One unified interface for all providers\n",
    "3. **Handle Failures** - Implement retries with exponential backoff\n",
    "4. **Understand Pydantic** - Python's data validation library\n",
    "5. **Get Structured Outputs** - Guaranteed JSON responses from LLMs\n",
    "6. **Use Async** - Make concurrent API calls for speed\n",
    "7. **Use Instructor** - Alternative approach to structured outputs\n",
    "8. **Generate Images** - Create images with Google Imagen\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Setup: API Keys and Environment\n",
    "\n",
    "Before we can call any LLM, we need API keys. These are like passwords that identify you to the service.\n",
    "\n",
    "## 1.1 Getting API Keys\n",
    "\n",
    "| Provider | Where to Get Key | Cost |\n",
    "|----------|-----------------|------|\n",
    "| **OpenAI** | [platform.openai.com](https://platform.openai.com) | Pay-as-you-go |\n",
    "| **Anthropic** | [console.anthropic.com](https://console.anthropic.com) | Pay-as-you-go |\n",
    "| **Google** | [aistudio.google.com](https://aistudio.google.com) | Free tier available |\n",
    "\n",
    "## 1.2 Storing Keys Securely\n",
    "\n",
    "Never put API keys in your code! Instead, use a `.env` file (in your project root)\n",
    "```python\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "Make sure `.env` is in your `.gitignore` so you don't accidentally commit your keys!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key loaded: True\n",
      "Anthropic key loaded: False\n",
      "Google key loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Suppress noisy warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify keys are loaded (don't print actual keys!)\n",
    "print(\"OpenAI key loaded:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"Anthropic key loaded:\", \"ANTHROPIC_API_KEY\" in os.environ)\n",
    "print(\"Google key loaded:\", \"GOOGLE_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Provider SDKs: The Native Way\n",
    "\n",
    "Each LLM provider has their own Python SDK. Let's see how each one works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 OpenAI SDK\n",
    "\n",
    "OpenAI's SDK is the most widely used. It powers the GPT-4.1 series and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A***********************************************************************************************************************************************************************7GYA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m openai_client = OpenAI()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Make a chat completion request\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-5-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is Python in exactly one sentence?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Extract the response\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOpenAI Response:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\ai-engineering-fordham\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A***********************************************************************************************************************************************************************7GYA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create a client (automatically uses OPENAI_API_KEY from environment)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Make a chat completion request\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python in exactly one sentence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "print(\"OpenAI Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Response Structure\n",
    "\n",
    "The response is a `ChatCompletion` object. Key parts:\n",
    "\n",
    "```python\n",
    "response.choices[0].message.content  # The actual text response\n",
    "response.choices[0].finish_reason    # Why generation stopped (\"stop\", \"length\", etc.)\n",
    "response.usage.total_tokens          # Tokens used (for billing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Anthropic SDK\n",
    "\n",
    "Anthropic makes Claude models. Their API is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "# Create a client\n",
    "anthropic_client = Anthropic()\n",
    "\n",
    "# Make a message request (note: different method name!)\n",
    "response = anthropic_client.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=1024,  # Required for Anthropic!\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python in exactly one sentence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the response (different structure!)\n",
    "print(\"Anthropic Response:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic Response Structure\n",
    "\n",
    "Different from OpenAI:\n",
    "\n",
    "```python\n",
    "response.content[0].text    # The actual text (not .message.content!)\n",
    "response.stop_reason        # Why it stopped\n",
    "response.usage.input_tokens # Tokens used\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Google Gemini SDK\n",
    "\n",
    "Google's SDK is different again. They use the `google-genai` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Response:\n",
      "Python is a high-level, general-purpose, interpreted programming language known for its clear, readable syntax and extensive libraries, enabling rapid development across various domains.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Create a client\n",
    "google_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Generate content (yet another API style!)\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is Python in exactly one sentence?\"\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "print(\"Google Response:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The Problem: Three APIs, Three Formats\n",
    "\n",
    "Notice how each provider has:\n",
    "- Different client initialization\n",
    "- Different method names (`chat.completions.create` vs `messages.create` vs `generate_content`)\n",
    "- Different response structures\n",
    "- Different parameter names\n",
    "\n",
    "This makes it hard to:\n",
    "- Switch providers\n",
    "- Compare models\n",
    "- Write reusable code\n",
    "\n",
    "**Solution: LiteLLM** - One API to rule them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. LiteLLM: Unified Interface\n",
    "\n",
    "LiteLLM provides a single, consistent API that works with 100+ LLM providers.\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **One API** | Same code for any provider |\n",
    "| **Easy Switching** | Change one line to switch models |\n",
    "| **Fallbacks** | Automatically try another provider if one fails |\n",
    "| **Cost Tracking** | Built-in usage monitoring |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Same function works with ANY provider!\n",
    "def ask_llm(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    \"\"\"Ask any LLM a question using LiteLLM.\"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "print(\"GPT-5-mini:\", ask_llm(\"Say 'hello' in French\", model=\"gpt-5-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic (same code, different model!)\n",
    "print(\"Claude Haiku:\", ask_llm(\"Say 'hello' in French\", model=\"claude-3-5-haiku-20241022\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ask_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Google (same code, just add \"gemini/\" prefix)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGemini Flash:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mask_llm\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mSay \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhello\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in French\u001b[39m\u001b[33m\"\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mgemini/gemini-3-flash-preview\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'ask_llm' is not defined"
     ]
    }
   ],
   "source": [
    "# Google (same code, just add \"gemini/\" prefix)\n",
    "print(\"Gemini Flash:\", ask_llm(\"Say 'hello' in French\", model=\"gemini/gemini-2.5-flash\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Name Reference\n",
    "\n",
    "| Provider | Model Name | Notes |\n",
    "|----------|-----------|-------|\n",
    "| **OpenAI** | `gpt-5.2` | Most capable (latest) |\n",
    "| | `gpt-5.2-pro` | More compute, better answers |\n",
    "| | `gpt-5.1` | Great for coding/agentic tasks |\n",
    "| | `gpt-5-mini` | Fast and cheap |\n",
    "| **Anthropic** | `claude-opus-4-5-20251101` | Most capable |\n",
    "| | `claude-sonnet-4-5-20250929` | Best balance |\n",
    "| | `claude-3-5-haiku-20241022` | Fast and cheap |\n",
    "| **Google** | `gemini/gemini-2.5-pro` | Most capable |\n",
    "| | `gemini/gemini-2.5-flash` | Fast and cheap |\n",
    "\n",
    "**Note**: We use `gpt-5-mini` in examples because it's cost-effective for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Messages and Conversation History\n",
    "\n",
    "LLMs understand different \"roles\" in a conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        # System message: sets the behavior/personality\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful pirate. Always respond like a pirate.\"},\n",
    "        # User message: the actual question\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Making API Calls Robust: Retries\n",
    "\n",
    "API calls can fail for many reasons:\n",
    "- **Rate limiting** (HTTP 429) - too many requests\n",
    "- **Server errors** (HTTP 500) - provider issues\n",
    "- **Timeouts** - network issues\n",
    "\n",
    "**Solution:** Retry with exponential backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Manual Retry Pattern\n",
    "\n",
    "Here's how to implement retries from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def call_with_retries(prompt: str, max_retries: int = 5, base_delay: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Call LLM with exponential backoff retry.\n",
    "    \n",
    "    Wait times: 1s -> 2s -> 4s -> 8s -> 16s (plus random jitter)\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = litellm.completion(\n",
    "                model=\"gpt-5-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                raise  # Give up after max retries\n",
    "            \n",
    "            # Exponential backoff with jitter\n",
    "            delay = base_delay * (2 ** (attempt - 1)) + random.random()\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "# Test it\n",
    "result = call_with_retries(\"Say 'hello'\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using Tenacity Library\n",
    "\n",
    "Writing retry logic is tedious. The `tenacity` library makes it elegant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),           # Max 5 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=60),  # Exponential backoff\n",
    "    reraise=True                          # Re-raise the exception if all retries fail\n",
    ")\n",
    "def robust_llm_call(prompt: str, model: str = \"gpt-5-mini\") -> str:\n",
    "    \"\"\"Call LLM with automatic retries.\"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# The decorator handles all retry logic!\n",
    "result = robust_llm_call(\"What is 2 + 2?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Built-in Client Retries\n",
    "\n",
    "The OpenAI client has built-in retry support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create client with automatic retries\n",
    "client = OpenAI(\n",
    "    max_retries=5,  # Automatically retry up to 5 times\n",
    "    timeout=30.0    # Timeout after 30 seconds\n",
    ")\n",
    "\n",
    "# Now all calls through this client will automatically retry!\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. What is Pydantic?\n",
    "\n",
    "**Pydantic** is Python's most popular data validation library. It lets you:\n",
    "\n",
    "- Define data structures with types\n",
    "- Automatically validate data\n",
    "- Get helpful error messages\n",
    "- Serialize to/from JSON\n",
    "\n",
    "It's used everywhere in modern Python: FastAPI, LangChain, Django Ninja, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Basic Pydantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a data structure\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int = Field(ge=0, le=150)  # Must be 0-150\n",
    "    email: str | None = None        # Optional field\n",
    "\n",
    "# Create an instance - Pydantic validates automatically!\n",
    "person = Person(name=\"Alice\", age=30, email=\"alice@example.com\")\n",
    "print(person)\n",
    "print(f\"Name: {person.name}, Age: {person.age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic catches invalid data\n",
    "try:\n",
    "    invalid_person = Person(name=\"Bob\", age=200)  # Age > 150!\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic auto-converts types when possible\n",
    "person = Person(name=\"Charlie\", age=\"25\")  # String \"25\" -> int 25\n",
    "print(f\"Age is {person.age}, type: {type(person.age)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Serialization (to/from JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSON\n",
    "person = Person(name=\"Diana\", age=28)\n",
    "json_str = person.model_dump_json(indent=2)\n",
    "print(\"As JSON:\")\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse from JSON\n",
    "json_data = '{\"name\": \"Eve\", \"age\": 35, \"email\": \"eve@example.com\"}'\n",
    "person = Person.model_validate_json(json_data)\n",
    "print(f\"Parsed: {person.name}, {person.age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Why Pydantic Matters for LLMs\n",
    "\n",
    "LLMs return free-form text. Pydantic lets us:\n",
    "\n",
    "1. **Define what we want** - Create a schema/model\n",
    "2. **Get structured data** - LLM returns JSON matching our schema\n",
    "3. **Validate automatically** - Pydantic checks the data is correct\n",
    "4. **Use easily** - Access fields with dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Structured Outputs with Pydantic\n",
    "\n",
    "The **problem**: LLMs return free-form text that's hard to parse.\n",
    "\n",
    "Ask \"extract info from this review\" and you might get:\n",
    "- \"The sentiment is positive and the rating is 4.5\"\n",
    "- \"Rating: 4.5/5, Sentiment: positive\"\n",
    "- \"Positive review! 4.5 stars.\"\n",
    "\n",
    "**Solution**: Use `response_format` to get guaranteed JSON structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Define the structure we want\n",
    "class MovieReview(BaseModel):\n",
    "    \"\"\"Structured data extracted from a movie review.\"\"\"\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field(\n",
    "        description=\"The overall sentiment of the review\"\n",
    "    )\n",
    "    rating: float = Field(\n",
    "        description=\"Numeric rating from 1.0 to 5.0\",\n",
    "        ge=1.0,\n",
    "        le=5.0\n",
    "    )\n",
    "    key_points: list[str] = Field(\n",
    "        description=\"Main points mentioned in the review (1-5 items)\",\n",
    "        min_length=1,\n",
    "        max_length=5\n",
    "    )\n",
    "    reviewer_name: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Name of the reviewer if mentioned\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'litellm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m review_text = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mThis movie was absolutely fantastic! The cinematography was stunning, \u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mand the acting performances were top-notch. I\u001b[39m\u001b[33m'\u001b[39m\u001b[33md give it 4.5 stars. \u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mThe plot kept me engaged from start to finish. Highly recommend!\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m- Sarah Johnson\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Use LiteLLM with response_format\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response = \u001b[43mlitellm\u001b[49m.completion(\n\u001b[32m     11\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-5-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     messages=[\n\u001b[32m     13\u001b[39m         {\n\u001b[32m     14\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mExtract structured information from movie reviews.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m         },\n\u001b[32m     17\u001b[39m         {\n\u001b[32m     18\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtract information from this review:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mreview_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m         }\n\u001b[32m     21\u001b[39m     ],\n\u001b[32m     22\u001b[39m     response_format=MovieReview  \u001b[38;5;66;03m# Tell the LLM to return this structure!\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Parse the JSON response into our Pydantic model\u001b[39;00m\n\u001b[32m     26\u001b[39m review_data = MovieReview.model_validate_json(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'litellm' is not defined"
     ]
    }
   ],
   "source": [
    "# A sample review to analyze\n",
    "review_text = \"\"\"\n",
    "This movie was absolutely fantastic! The cinematography was stunning, \n",
    "and the acting performances were top-notch. I'd give it 4.5 stars. \n",
    "The plot kept me engaged from start to finish. Highly recommend!\n",
    "- Sarah Johnson\n",
    "\"\"\"\n",
    "\n",
    "# Use LiteLLM with response_format\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Extract structured information from movie reviews.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Extract information from this review:\\n\\n{review_text}\"\n",
    "        }\n",
    "    ],\n",
    "    response_format=MovieReview  # Tell the LLM to return this structure!\n",
    ")\n",
    "\n",
    "# Parse the JSON response into our Pydantic model\n",
    "review_data = MovieReview.model_validate_json(response.choices[0].message.content)\n",
    "\n",
    "# Now we have clean, typed data!\n",
    "print(f\"Sentiment: {review_data.sentiment}\")\n",
    "print(f\"Rating: {review_data.rating}\")\n",
    "print(f\"Key Points: {review_data.key_points}\")\n",
    "print(f\"Reviewer: {review_data.reviewer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Nested Models\n",
    "\n",
    "You can create complex structures with nested Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(BaseModel):\n",
    "    \"\"\"Information about an actor.\"\"\"\n",
    "    name: str\n",
    "    role: str\n",
    "\n",
    "class MovieInfo(BaseModel):\n",
    "    \"\"\"Comprehensive movie information.\"\"\"\n",
    "    title: str\n",
    "    year: int = Field(ge=1900, le=2030)\n",
    "    genre: list[str]\n",
    "    director: str\n",
    "    actors: list[Actor]  # Nested model!\n",
    "    plot_summary: str = Field(max_length=500)\n",
    "\n",
    "# Use it\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Give me information about the movie Inception\"}\n",
    "    ],\n",
    "    response_format=MovieInfo\n",
    ")\n",
    "\n",
    "movie = MovieInfo.model_validate_json(response.choices[0].message.content)\n",
    "print(f\"Title: {movie.title} ({movie.year})\")\n",
    "print(f\"Director: {movie.director}\")\n",
    "print(f\"Genres: {', '.join(movie.genre)}\")\n",
    "print(f\"\\nActors:\")\n",
    "for actor in movie.actors:\n",
    "    print(f\"  - {actor.name} as {actor.role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Async Programming\n",
    "\n",
    "**Problem**: If you need to make 100 LLM calls, doing them one-by-one is slow.\n",
    "\n",
    "**Solution**: Make them concurrently with async programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 What is Async?\n",
    "\n",
    "Think of it like ordering at a restaurant:\n",
    "\n",
    "- **Synchronous**: Order one dish, wait for it, eat it, then order the next\n",
    "- **Asynchronous**: Order all dishes at once, they arrive as they're ready\n",
    "\n",
    "Key Python concepts:\n",
    "\n",
    "| Keyword | Meaning |\n",
    "|---------|---------|\n",
    "| `async def` | Defines an async function |\n",
    "| `await` | Pause here until the result is ready |\n",
    "| `asyncio.gather()` | Run multiple async tasks concurrently |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Sequential vs Concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# A list of prompts to process\n",
    "prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the capital of Germany?\",\n",
    "    \"What is the capital of Italy?\",\n",
    "    \"What is the capital of Spain?\",\n",
    "    \"What is the capital of Portugal?\",\n",
    "    \"What is the capital of Greece?\",\n",
    "    \"What is the capital of Turkey?\",\n",
    "    \"What is the capital of Bulgaria?\",\n",
    "    \"What is the capital of Romania?\",\n",
    "    \"What is the capital of Hungary?\",\n",
    "    \"What is the capital of Poland?\",\n",
    "    \"What is the capital of Czech Republic?\",\n",
    "]\n",
    "\n",
    "# Sequential: one at a time\n",
    "start = time.time()\n",
    "sequential_results = []\n",
    "for prompt in prompts:\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    sequential_results.append(response.choices[0].message.content)\n",
    "sequential_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential: {sequential_time:.2f} seconds\")\n",
    "for prompt, result in zip(prompts, sequential_results):\n",
    "    print(f\"  {prompt} -> {result[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Async function to call LLM\n",
    "async def async_ask(prompt: str) -> str:\n",
    "    \"\"\"Make an async LLM call.\"\"\"\n",
    "    response = await litellm.acompletion(  # Note: acompletion, not completion!\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Async function to process all prompts concurrently\n",
    "async def process_all(prompts: list[str]) -> list[str]:\n",
    "    \"\"\"Process all prompts concurrently.\"\"\"\n",
    "    tasks = [async_ask(prompt) for prompt in prompts]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Run concurrently\n",
    "start = time.time()\n",
    "concurrent_results = await process_all(prompts)\n",
    "concurrent_time = time.time() - start\n",
    "\n",
    "print(f\"Concurrent: {concurrent_time:.2f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / concurrent_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 When to Use Async\n",
    "\n",
    "Use async when:\n",
    "- Processing many items (batch analysis)\n",
    "- Making independent API calls\n",
    "- Building web applications (FastAPI uses async)\n",
    "\n",
    "Don't bother for:\n",
    "- Single API calls\n",
    "- Sequential workflows where each step depends on the previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Instructor Library\n",
    "\n",
    "**Instructor** is another approach to structured outputs. It:\n",
    "\n",
    "- Patches existing clients (OpenAI, Anthropic, etc.)\n",
    "- Returns Pydantic objects directly (not JSON strings)\n",
    "- Has built-in retry and validation\n",
    "- Works with multiple providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Patch the OpenAI client with Instructor\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Define what we want\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    occupation: str\n",
    "\n",
    "# Use it - returns a Pydantic object directly!\n",
    "user = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a random person's info\"}\n",
    "    ],\n",
    "    response_model=UserInfo  # Instructor uses response_model, not response_format\n",
    ")\n",
    "\n",
    "# user is already a UserInfo object!\n",
    "print(f\"Name: {user.name}\")\n",
    "print(f\"Age: {user.age}\")\n",
    "print(f\"Occupation: {user.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Instructor with Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "# Patch Anthropic client\n",
    "anthropic_instructor = instructor.from_anthropic(Anthropic())\n",
    "\n",
    "# Same interface!\n",
    "user = anthropic_instructor.messages.create(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a random person's info\"}\n",
    "    ],\n",
    "    response_model=UserInfo\n",
    ")\n",
    "\n",
    "print(f\"From Claude: {user.name}, {user.age}, {user.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Instructor vs LiteLLM: When to Use Each\n",
    "\n",
    "| Feature | LiteLLM | Instructor |\n",
    "|---------|---------|------------|\n",
    "| **Multi-provider** | Yes (100+ providers) | Yes (OpenAI, Anthropic, etc.) |\n",
    "| **Structured output** | Returns JSON string | Returns Pydantic object |\n",
    "| **Retry built-in** | No (use tenacity) | Yes |\n",
    "| **Validation retry** | No | Yes (retries if validation fails) |\n",
    "| **Learning curve** | Lower | Slightly higher |\n",
    "\n",
    "**Use LiteLLM when**: You want unified access to many providers\n",
    "\n",
    "**Use Instructor when**: You want the cleanest structured output experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Image Generation with Gemini (Nano Banana)\n",
    "\n",
    "Gemini models can generate images natively - no separate Imagen model needed! This feature is called \"Nano Banana\" internally at Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from pathlib import Path\n",
    "\n",
    "# Create client\n",
    "google_client = genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Generate an image using Nano Banana (Gemini's native image generation)\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",  # Nano Banana model\n",
    "    contents=[\"A cute robot learning to code, digital art style\"],\n",
    ")\n",
    "\n",
    "# Find and save the image from the response\n",
    "output_path = Path(\"temp/robot_coding.png\")\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "for part in response.parts:\n",
    "    if part.inline_data is not None:\n",
    "        image = part.as_image()\n",
    "        image.save(output_path)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image in Jupyter\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if output_path.exists():\n",
    "    display(Image(filename=str(output_path), width=400))\n",
    "else:\n",
    "    print(\"Run the previous cell first to generate the image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Nano Banana Pro (Gemini 3)\n",
    "\n",
    "For complex prompts that need reasoning, use **Nano Banana Pro**. It \"thinks\" before generating, making it great for:\n",
    "- Infographics with text\n",
    "- Complex compositions\n",
    "- Multi-element scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nano Banana Pro - uses \"thinking\" for complex prompts\n",
    "from google.genai import types\n",
    "\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-3-pro-image-preview\",  # Nano Banana Pro\n",
    "    contents=[\"Create a detailed infographic showing how neural networks learn, \"\n",
    "              \"with clear labels, arrows showing data flow, and a modern tech aesthetic\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE'],\n",
    "        thinking_config=types.ThinkingConfig(\n",
    "            include_thoughts=True  # Enable \"thinking\" mode\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the pro image\n",
    "pro_output_path = Path(\"temp/neural_network_infographic.png\")\n",
    "\n",
    "for part in response.parts:\n",
    "    if part.inline_data is not None:\n",
    "        image = part.as_image()\n",
    "        image.save(pro_output_path)\n",
    "        print(f\"Pro image saved to {pro_output_path}\")\n",
    "        break\n",
    "    elif part.text is not None:\n",
    "        print(f\"Model's thoughts: {part.text[:200]}...\")  # Show reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Pro image\n",
    "if pro_output_path.exists():\n",
    "    display(Image(filename=str(pro_output_path), width=500))\n",
    "else:\n",
    "    print(\"Run the previous cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Tips for Image Generation\n",
    "\n",
    "**Nano Banana models**:\n",
    "| Model | ID | Best For |\n",
    "|-------|-----|----------|\n",
    "| **Nano Banana** | `gemini-2.5-flash-image` | Fast, high-volume tasks |\n",
    "| **Nano Banana Pro** | `gemini-3-pro-image-preview` | Pro quality, complex prompts |\n",
    "\n",
    "**Good prompts**:\n",
    "- Be specific: \"A golden retriever puppy playing in autumn leaves, photograph\"\n",
    "- Include style: \"digital art\", \"oil painting\", \"photograph\", \"3D render\"\n",
    "- Describe lighting: \"sunset lighting\", \"studio lighting\"\n",
    "\n",
    "**With aspect ratio**:\n",
    "```python\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"your prompt\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['TEXT', 'IMAGE'],\n",
    "        image_config=types.ImageConfig(aspect_ratio=\"16:9\")\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "**Edit existing images**:\n",
    "```python\n",
    "from PIL import Image\n",
    "img = Image.open(\"photo.png\")\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"Add a wizard hat\", img],  # Pass image + edit instruction\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: All images include an invisible SynthID watermark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10. Summary\n",
    "\n",
    "Today we covered a lot! Here's a quick reference:\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|-------------|\n",
    "| **Provider SDKs** | Each provider has different APIs (OpenAI, Anthropic, Google) |\n",
    "| **LiteLLM** | Unified interface for all providers - use this! |\n",
    "| **Retries** | Use `tenacity` or built-in client retries for robustness |\n",
    "| **Pydantic** | Data validation library for defining schemas |\n",
    "| **Structured Outputs** | Use `response_format` for guaranteed JSON |\n",
    "| **Async** | Use `acompletion` + `asyncio.gather` for concurrent calls |\n",
    "| **Instructor** | Alternative for structured outputs with validation retries |\n",
    "| **Nano Banana** | Gemini's native image generation (`gemini-2.5-flash-image`) |\n",
    "\n",
    "## Quick Code Reference\n",
    "\n",
    "```python\n",
    "# Basic LLM call with LiteLLM\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "\n",
    "# Structured output\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[...],\n",
    "    response_format=MyPydanticModel\n",
    ")\n",
    "data = MyPydanticModel.model_validate_json(response.choices[0].message.content)\n",
    "\n",
    "# Async call\n",
    "response = await litellm.acompletion(model=\"gpt-5-mini\", messages=[...])\n",
    "\n",
    "# Image generation (Nano Banana)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-image\",\n",
    "    contents=[\"your prompt\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [LiteLLM Documentation](https://docs.litellm.ai)\n",
    "- [Pydantic Documentation](https://docs.pydantic.dev)\n",
    "- [Instructor Documentation](https://python.useinstructor.com)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs)\n",
    "- [Anthropic API Reference](https://docs.anthropic.com)\n",
    "- [Google Gemini API](https://ai.google.dev/gemini-api/docs)\n",
    "- [Tenacity Documentation](https://tenacity.readthedocs.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
